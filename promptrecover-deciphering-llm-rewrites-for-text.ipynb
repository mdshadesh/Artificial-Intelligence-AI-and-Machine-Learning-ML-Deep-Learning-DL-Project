{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10c57625",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-29T10:00:56.301603Z",
     "iopub.status.busy": "2024-03-29T10:00:56.301037Z",
     "iopub.status.idle": "2024-03-29T10:00:57.504588Z",
     "shell.execute_reply": "2024-03-29T10:00:57.502853Z"
    },
    "papermill": {
     "duration": 1.216172,
     "end_time": "2024-03-29T10:00:57.509126",
     "exception": false,
     "start_time": "2024-03-29T10:00:56.292954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/llm-prompt-recovery/sample_submission.csv\n",
      "/kaggle/input/llm-prompt-recovery/train.csv\n",
      "/kaggle/input/llm-prompt-recovery/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acf60a3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:00:57.522588Z",
     "iopub.status.busy": "2024-03-29T10:00:57.521870Z",
     "iopub.status.idle": "2024-03-29T10:00:59.545205Z",
     "shell.execute_reply": "2024-03-29T10:00:59.543527Z"
    },
    "papermill": {
     "duration": 2.034129,
     "end_time": "2024-03-29T10:00:59.548905",
     "exception": false,
     "start_time": "2024-03-29T10:00:57.514776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0182bb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:00:59.561963Z",
     "iopub.status.busy": "2024-03-29T10:00:59.561431Z",
     "iopub.status.idle": "2024-03-29T10:00:59.605244Z",
     "shell.execute_reply": "2024-03-29T10:00:59.603930Z"
    },
    "papermill": {
     "duration": 0.054224,
     "end_time": "2024-03-29T10:00:59.608534",
     "exception": false,
     "start_time": "2024-03-29T10:00:59.554310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_df = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/train.csv\")\n",
    "test_df = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/test.csv\")\n",
    "sample_submission = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e3bd832",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:00:59.622897Z",
     "iopub.status.busy": "2024-03-29T10:00:59.621492Z",
     "iopub.status.idle": "2024-03-29T10:00:59.632349Z",
     "shell.execute_reply": "2024-03-29T10:00:59.631187Z"
    },
    "papermill": {
     "duration": 0.020616,
     "end_time": "2024-03-29T10:00:59.634837",
     "exception": false,
     "start_time": "2024-03-29T10:00:59.614221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "X_train = train_df['original_text']\n",
    "y_train = train_df['rewrite_prompt']\n",
    "X_test = test_df['original_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6232e56e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:00:59.648792Z",
     "iopub.status.busy": "2024-03-29T10:00:59.647147Z",
     "iopub.status.idle": "2024-03-29T10:00:59.680052Z",
     "shell.execute_reply": "2024-03-29T10:00:59.678290Z"
    },
    "papermill": {
     "duration": 0.043322,
     "end_time": "2024-03-29T10:00:59.683464",
     "exception": false,
     "start_time": "2024-03-29T10:00:59.640142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vectorize the text data\n",
    "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 3))\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "163357ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:00:59.697127Z",
     "iopub.status.busy": "2024-03-29T10:00:59.696663Z",
     "iopub.status.idle": "2024-03-29T10:00:59.707729Z",
     "shell.execute_reply": "2024-03-29T10:00:59.706182Z"
    },
    "papermill": {
     "duration": 0.02332,
     "end_time": "2024-03-29T10:00:59.712611",
     "exception": false,
     "start_time": "2024-03-29T10:00:59.689291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Convert this into a sea shanty: \"\"\"The competition dataset comprises text passages that have been rewritten by the Gemma LLM according to some rewrite_prompt instruction. The goal of the competition is to determine what prompt was used to rewrite each original text.  Please note that this is a Code Competition. When your submission is scored, this example test data will be replaced with the full test set. Expect roughly 2,000 original texts in the test set.\"\"\"']\n"
     ]
    }
   ],
   "source": [
    "unique_prompts = train_df['rewrite_prompt'].unique()\n",
    "print(unique_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35791f22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:00:59.727154Z",
     "iopub.status.busy": "2024-03-29T10:00:59.726655Z",
     "iopub.status.idle": "2024-03-29T10:01:00.365157Z",
     "shell.execute_reply": "2024-03-29T10:01:00.363764Z"
    },
    "papermill": {
     "duration": 0.649986,
     "end_time": "2024-03-29T10:01:00.368680",
     "exception": false,
     "start_time": "2024-03-29T10:00:59.718694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/train.csv\")\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/test.csv\")\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = train_data['original_text']\n",
    "y_train = train_data['rewrite_prompt']\n",
    "X_test = test_data['original_text']\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 3))\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_train_vec, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "test_preds = classifier.predict(X_test_vec)\n",
    "\n",
    "# Create submission file\n",
    "submission = test_data[['id']].copy()\n",
    "submission['rewrite_prompt'] = test_preds\n",
    "\n",
    "# Save submission file to the output directory\n",
    "submission.to_csv(\"/kaggle/working/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7ba3f97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:01:00.381979Z",
     "iopub.status.busy": "2024-03-29T10:01:00.381482Z",
     "iopub.status.idle": "2024-03-29T10:01:00.397616Z",
     "shell.execute_reply": "2024-03-29T10:01:00.396152Z"
    },
    "papermill": {
     "duration": 0.026653,
     "end_time": "2024-03-29T10:01:00.400864",
     "exception": false,
     "start_time": "2024-03-29T10:01:00.374211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset:\n",
      "   id                                      original_text  \\\n",
      "0  -1  The competition dataset comprises text passage...   \n",
      "\n",
      "                                      rewrite_prompt  \\\n",
      "0  Convert this into a sea shanty: \"\"\"The competi...   \n",
      "\n",
      "                                      rewritten_text  \n",
      "0  Here is your shanty: (Verse 1) The text is rew...  \n",
      "\n",
      "Test Dataset:\n",
      "   id                                      original_text  \\\n",
      "0  -1  The competition dataset comprises text passage...   \n",
      "\n",
      "                                      rewritten_text  \n",
      "0  Here is your shanty: (Verse 1) The text is rew...  \n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the training dataset\n",
    "print(\"Training Dataset:\")\n",
    "print(train_data.head())\n",
    "\n",
    "# Display the first few rows of the test dataset\n",
    "print(\"\\nTest Dataset:\")\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2d1da90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:01:00.417127Z",
     "iopub.status.busy": "2024-03-29T10:01:00.416252Z",
     "iopub.status.idle": "2024-03-29T10:01:00.425386Z",
     "shell.execute_reply": "2024-03-29T10:01:00.423882Z"
    },
    "papermill": {
     "duration": 0.020145,
     "end_time": "2024-03-29T10:01:00.428423",
     "exception": false,
     "start_time": "2024-03-29T10:01:00.408278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Headers:\n",
      "Index(['id', 'original_text', 'rewrite_prompt', 'rewritten_text'], dtype='object')\n",
      "\n",
      "Test Dataset Headers:\n",
      "Index(['id', 'original_text', 'rewritten_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Display the headers of the training dataset\n",
    "print(\"Training Dataset Headers:\")\n",
    "print(train_data.columns)\n",
    "\n",
    "# Display the headers of the test dataset\n",
    "print(\"\\nTest Dataset Headers:\")\n",
    "print(test_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4d7199",
   "metadata": {
    "papermill": {
     "duration": 0.005333,
     "end_time": "2024-03-29T10:01:00.439349",
     "exception": false,
     "start_time": "2024-03-29T10:01:00.434016",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "1. **Project Overview**: The goal of the project is to recover the prompt used to rewrite a given text. In other words, you're given original texts that have been rewritten by a language model (LLM) according to some rewrite prompt instruction. Your task is to determine what prompt was used to rewrite each original text.\n",
    "\n",
    "2. **Dataset**: The dataset provided consists of two main files: `train.csv` and `test.csv`. The `train.csv` file contains original texts along with their rewritten versions and the corresponding rewrite prompts. The `test.csv` file contains only original texts for which you need to predict the rewrite prompts.\n",
    "\n",
    "3. **Algorithm**: The algorithm used in this project is a supervised machine learning approach. Since the task is to classify the rewrite prompts based on the original texts, it falls under the category of text classification. Commonly used algorithms for text classification include:\n",
    "   - Logistic Regression\n",
    "   - Random Forest\n",
    "   - Support Vector Machines (SVM)\n",
    "   - Gradient Boosting\n",
    "   - Neural Networks (e.g., LSTM, BERT)\n",
    "\n",
    "4. **Approach**:\n",
    "   - **Data Preprocessing**: The original texts are preprocessed, which may include tasks such as tokenization, removing stopwords, and vectorization (e.g., TF-IDF or word embeddings).\n",
    "   - **Feature Engineering**: Features are extracted from the preprocessed texts to represent them numerically.\n",
    "   - **Model Training**: A classification model (e.g., Random Forest, Logistic Regression) is trained using the preprocessed data, with the rewrite prompts as the target variable.\n",
    "   - **Model Evaluation**: The trained model is evaluated using appropriate metrics (e.g., accuracy, F1-score) to assess its performance on the validation set.\n",
    "   - **Prediction**: The trained model is then used to predict the rewrite prompts for the original texts in the test dataset.\n",
    "   - **Submission**: The predicted rewrite prompts are submitted as the final output for evaluation.\n",
    "\n",
    "5. **Evaluation Metric**: The competition might specify an evaluation metric to measure the performance of the predictions. Common metrics for classification tasks include accuracy, precision, recall, and F1-score.\n",
    "\n",
    "6. **Iterative Process**: You might iterate on the steps mentioned above, trying different algorithms, feature engineering techniques, and hyperparameters to improve the model's performance.\n",
    "\n",
    "7. **Submission**: Once you're satisfied with the performance of your model on the validation set, you submit your predictions for the test dataset. The competition organizers evaluate your predictions against the ground truth and provide a score based on the specified evaluation metric.\n",
    "\n",
    "In summary, this project involves using supervised machine learning techniques to classify rewrite prompts based on original texts, and various algorithms such as Random Forest or Logistic Regression can be used for this task. The choice of algorithm and the success of the project depend on factors such as data quality, feature engineering, and model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae78a7d2",
   "metadata": {
    "papermill": {
     "duration": 0.006017,
     "end_time": "2024-03-29T10:01:00.450987",
     "exception": false,
     "start_time": "2024-03-29T10:01:00.444970",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Introduction:\n",
    "The project focuses on recovering the prompts used to rewrite given texts, a task crucial in natural language processing (NLP) workflows. It presents a machine learning competition aimed at exploring effective prompt engineering for large language models (LLMs). The dataset comprises original texts paired with their rewritten versions generated by Gemma, a family of open LLMs developed by Google. Participants are challenged to determine the prompts used in the rewriting process.\n",
    "\n",
    "### Methodology:\n",
    "1. **Data Preparation**: The dataset consists of original texts and their corresponding rewritten versions, paired with rewrite prompts. Both the original and rewritten texts are preprocessed for further analysis.\n",
    "\n",
    "2. **Feature Engineering**: Textual features are extracted from the preprocessed data. Techniques like TF-IDF vectorization or word embeddings may be employed to represent the text data numerically.\n",
    "\n",
    "3. **Model Selection**: Supervised learning algorithms such as Logistic Regression, Random Forest, or Gradient Boosting are considered for prompt recovery. Models are trained on the preprocessed dataset, with the rewrite prompts as the target variable.\n",
    "\n",
    "4. **Model Evaluation**: The trained models are evaluated using appropriate metrics like accuracy or F1-score. Cross-validation techniques may be employed to ensure robust performance assessment.\n",
    "\n",
    "5. **Hyperparameter Tuning**: Hyperparameters of the selected models are fine-tuned to optimize performance on the validation set.\n",
    "\n",
    "6. **Prediction and Submission**: The best-performing model is used to predict rewrite prompts for the test dataset. The predictions are then submitted for evaluation against the ground truth.\n",
    "\n",
    "### Results Discussion:\n",
    "The performance of the models is analyzed based on evaluation metrics specified by the competition organizers. The results are discussed in terms of the effectiveness of various algorithms, feature engineering techniques, and prompt recovery strategies. Insights into the challenges encountered and successful methodologies employed during the competition are highlighted.\n",
    "\n",
    "### Conclusion:\n",
    "The project concludes with a summary of key findings and lessons learned. It discusses the implications of the results for NLP workflows and prompts further research avenues in prompt engineering for LLMs. The competition serves as a platform for exploring innovative approaches to text rewriting tasks and contributes to advancing the field of natural language processing.\n",
    "\n",
    "### Consultation:\n",
    "For further inquiries or consultation regarding the project methodology, results, or implications, feel free to reach out to the project team or experts in the field of natural language processing. Collaboration opportunities, research partnerships, or discussions on related topics can also be explored to deepen the understanding of the findings and foster knowledge exchange within the community."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7806901,
     "sourceId": 67121,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.750859,
   "end_time": "2024-03-29T10:01:01.384364",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-29T10:00:52.633505",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
