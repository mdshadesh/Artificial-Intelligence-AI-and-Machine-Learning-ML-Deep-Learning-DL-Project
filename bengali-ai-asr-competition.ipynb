{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ae03807",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-16T06:42:06.425382Z",
     "iopub.status.busy": "2023-10-16T06:42:06.424644Z",
     "iopub.status.idle": "2023-10-16T06:42:30.271086Z",
     "shell.execute_reply": "2023-10-16T06:42:30.269926Z"
    },
    "papermill": {
     "duration": 23.852537,
     "end_time": "2023-10-16T06:42:30.273300",
     "exception": false,
     "start_time": "2023-10-16T06:42:06.420763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat '/kaggle/input/bengali-eval-data/predict.py': No such file or directory\r\n",
      "jiwer/\r\n",
      "jiwer/jiwer-2.3.0-py3-none-any.whl\r\n",
      "jiwer/python-Levenshtein-0.12.2.tar.gz\r\n",
      "jiwer/setuptools-65.3.0-py3-none-any.whl\r\n",
      "Looking in links: ./\r\n",
      "Processing ./jiwer/python-Levenshtein-0.12.2.tar.gz\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from python-Levenshtein==0.12.2) (59.8.0)\r\n",
      "Building wheels for collected packages: python-Levenshtein\r\n",
      "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp310-cp310-linux_x86_64.whl size=79704 sha256=f1c058b22f21030cd9aa89906c949bf04fdc37e8be736494ca469403a0f60c74\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/f1/ab/b1/90d2068d73d15e52c1a65676d269a9f043b61221a29f7298e7\r\n",
      "Successfully built python-Levenshtein\r\n",
      "Installing collected packages: python-Levenshtein\r\n",
      "  Attempting uninstall: python-Levenshtein\r\n",
      "    Found existing installation: python-Levenshtein 0.21.1\r\n",
      "    Uninstalling python-Levenshtein-0.21.1:\r\n",
      "      Successfully uninstalled python-Levenshtein-0.21.1\r\n",
      "Successfully installed python-Levenshtein-0.12.2\r\n",
      "Looking in links: ./\r\n",
      "Processing ./jiwer/jiwer-2.3.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: python-Levenshtein==0.12.2 in /opt/conda/lib/python3.10/site-packages (from jiwer==2.3.0) (0.12.2)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from python-Levenshtein==0.12.2->jiwer==2.3.0) (59.8.0)\r\n",
      "Installing collected packages: jiwer\r\n",
      "Successfully installed jiwer-2.3.0\r\n"
     ]
    }
   ],
   "source": [
    "!cp /kaggle/input/bengali-eval-data/predict.py .\n",
    "\n",
    "!cp -r ../input/python-packages2 ./\n",
    "!tar xvfz ./python-packages2/jiwer.tgz\n",
    "!pip install ./jiwer/python-Levenshtein-0.12.2.tar.gz -f ./ --no-index\n",
    "!pip install ./jiwer/jiwer-2.3.0-py3-none-any.whl -f ./ --no-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8941b3f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T06:42:30.280542Z",
     "iopub.status.busy": "2023-10-16T06:42:30.280251Z",
     "iopub.status.idle": "2023-10-16T06:43:13.908426Z",
     "shell.execute_reply": "2023-10-16T06:43:13.905398Z"
    },
    "papermill": {
     "duration": 43.636728,
     "end_time": "2023-10-16T06:43:13.912945",
     "exception": false,
     "start_time": "2023-10-16T06:42:30.276217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.30.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import glob\n",
    "\n",
    "MODEL = '/kaggle/input/bengali-ai-asr-submission/bengali-whisper-medium/'\n",
    "PUNCT_MODELS = [\n",
    "    '/kaggle/input/bengali-ai-asr-submission/punct-model-6layers/',\n",
    "    '/kaggle/input/bengali-ai-asr-submission/punct-model-8layers/',\n",
    "    '/kaggle/input/bengali-ai-asr-submission/punct-model-11layers/',\n",
    "    '/kaggle/input/bengali-ai-asr-submission/punct-model-12layers/'\n",
    "]\n",
    "CHUNK_LENGTH_S = 20.1\n",
    "ENABLE_BEAM = True\n",
    "# none alone 0.9 0.037964275588318684\n",
    "# none alone 0.7 0.03592288063510065\n",
    "# none alone 0.4 0.03416501275871846\n",
    "PUNCT_WEIGHTS = [[1.0, 1.4, 1.0, 0.8]]\n",
    "\n",
    "if ENABLE_BEAM:\n",
    "    BATCH_SIZE = 4\n",
    "else:\n",
    "    BATCH_SIZE = 8\n",
    "\n",
    "if len(glob.glob(\"/kaggle/input/bengaliai-speech/test_mp3s/*.mp3\")) > 10:\n",
    "    EVAL = False\n",
    "    DATASET_PATH = '/kaggle/input/bengaliai-speech/test_mp3s/'\n",
    "else:\n",
    "    EVAL = True\n",
    "    DATASET_PATH = '/kaggle/input/bengaliai-speech/test_mp3s/'\n",
    "    \n",
    "import csv\n",
    "import glob\n",
    "import shutil\n",
    "import librosa\n",
    "import argparse\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "files = list(glob.glob(DATASET_PATH + '/' + '*.wav'))\n",
    "files += list(glob.glob(DATASET_PATH + '/' + '*.mp3'))\n",
    "files.sort()\n",
    "\n",
    "pipe = pipeline(task=\"automatic-speech-recognition\",\n",
    "                model=MODEL,\n",
    "                tokenizer=MODEL,\n",
    "                chunk_length_s=CHUNK_LENGTH_S, device=0, batch_size=BATCH_SIZE)\n",
    "pipe.model.config.forced_decoder_ids = pipe.tokenizer.get_decoder_prompt_ids(language=\"bn\", task=\"transcribe\")\n",
    "\n",
    "print(\"model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "856c836c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T06:43:13.926223Z",
     "iopub.status.busy": "2023-10-16T06:43:13.924348Z",
     "iopub.status.idle": "2023-10-16T06:43:13.933832Z",
     "shell.execute_reply": "2023-10-16T06:43:13.933024Z"
    },
    "papermill": {
     "duration": 0.017125,
     "end_time": "2023-10-16T06:43:13.935904",
     "exception": false,
     "start_time": "2023-10-16T06:43:13.918779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fix_repetition(text, max_count):\n",
    "    uniq_word_counter = {}\n",
    "    words = text.split()\n",
    "    for word in text.split():\n",
    "        if word not in uniq_word_counter:\n",
    "            uniq_word_counter[word] = 1\n",
    "        else:\n",
    "            uniq_word_counter[word] += 1\n",
    "\n",
    "    for word, count in uniq_word_counter.items():\n",
    "        if count > max_count:\n",
    "            words = [w for w in words if w != word]\n",
    "    text = \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d619d659",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T06:43:13.946936Z",
     "iopub.status.busy": "2023-10-16T06:43:13.946380Z",
     "iopub.status.idle": "2023-10-16T06:43:21.923683Z",
     "shell.execute_reply": "2023-10-16T06:43:21.922769Z"
    },
    "papermill": {
     "duration": 7.985147,
     "end_time": "2023-10-16T06:43:21.925779",
     "exception": false,
     "start_time": "2023-10-16T06:43:13.940632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if ENABLE_BEAM:\n",
    "    texts = pipe(files, generate_kwargs={\"max_length\": 260, \"num_beams\": 4})\n",
    "else:\n",
    "    texts = pipe(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b14721f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T06:43:21.933292Z",
     "iopub.status.busy": "2023-10-16T06:43:21.932438Z",
     "iopub.status.idle": "2023-10-16T06:43:49.811702Z",
     "shell.execute_reply": "2023-10-16T06:43:49.810715Z"
    },
    "papermill": {
     "duration": 27.885062,
     "end_time": "2023-10-16T06:43:49.813899",
     "exception": false,
     "start_time": "2023-10-16T06:43:21.928837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del pipe\n",
    "import torch\n",
    "models = [\n",
    "    AutoModelForTokenClassification.from_pretrained(f).eval().cuda() for f in PUNCT_MODELS\n",
    "]\n",
    "tokenizer = AutoTokenizer.from_pretrained(PUNCT_MODELS[0])\n",
    "def punctuate(text):\n",
    "    input_ids = tokenizer(text).input_ids\n",
    "    with torch.no_grad():\n",
    "        model = models[0]\n",
    "        logits = torch.nn.functional.softmax(\n",
    "            model(input_ids=torch.LongTensor([input_ids]).cuda()).logits[0, 1:-1],\n",
    "            dim=1).cpu()\n",
    "        for model in models[1:]:\n",
    "            logits += torch.nn.functional.softmax(\n",
    "                model(input_ids=torch.LongTensor([input_ids]).cuda()).logits[0, 1:-1],\n",
    "                dim=1).cpu()\n",
    "        logits = logits / len(models)\n",
    "        logits *= torch.FloatTensor(PUNCT_WEIGHTS)\n",
    "        label_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        tokens = tokenizer(text, add_special_tokens=False).input_ids\n",
    "        punct_text = \"\"\n",
    "        for index, token in enumerate(tokens):\n",
    "            token_str = tokenizer.decode(token)\n",
    "            if '##' not in token_str:\n",
    "                punct_text += \" \" + token_str\n",
    "            else:\n",
    "                punct_text += token_str[2:]\n",
    "            punct_text += ['', '।', ',', '?'][label_ids[index].item()]\n",
    "\n",
    "    punct_text = punct_text.strip()\n",
    "    return punct_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c59d22b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-16T06:43:49.821026Z",
     "iopub.status.busy": "2023-10-16T06:43:49.820740Z",
     "iopub.status.idle": "2023-10-16T06:43:49.924893Z",
     "shell.execute_reply": "2023-10-16T06:43:49.923532Z"
    },
    "papermill": {
     "duration": 0.109825,
     "end_time": "2023-10-16T06:43:49.926777",
     "exception": false,
     "start_time": "2023-10-16T06:43:49.816952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference finished!\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "with open(\"submission.csv\", 'wt', encoding=\"utf8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['id', 'sentence'])\n",
    "    for f, text in zip(files, texts):\n",
    "        file_id = Path(f).stem\n",
    "        pred = text['text'].strip()\n",
    "        pred = fix_repetition(pred, max_count=8)\n",
    "        pred = punctuate(pred)\n",
    "        if pred[-1] not in ['।', '?', ',']:\n",
    "            pred = pred + '।'\n",
    "        # print(i, file_id, pred)\n",
    "        prediction = [file_id, pred]\n",
    "        writer.writerow(prediction)\n",
    "        predictions.append(prediction)\n",
    "print(\"inference finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 114.473446,
   "end_time": "2023-10-16T06:43:53.207541",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-16T06:41:58.734095",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
