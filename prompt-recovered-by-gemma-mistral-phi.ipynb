{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b31b45b",
   "metadata": {
    "papermill": {
     "duration": 0.00923,
     "end_time": "2024-03-29T02:31:09.956092",
     "exception": false,
     "start_time": "2024-03-29T02:31:09.946862",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LLM Prompt Recovery using pretrained LLM\n",
    "This notebook explores the possibility of using pre-trained LLMs to recover the prompt used to generate a text.\n",
    "\n",
    "### References:\n",
    "- [h2oGPT Perplexity Ranking](https://www.kaggle.com/code/philippsinger/h2ogpt-perplexity-ranking)\n",
    "- [Prompt Prediction w/ Mixtral/Mistral7B/Gemma/Llama](https://www.kaggle.com/code/aatiffraz/prompt-prediction-w-mixtral-mistral7b-gemma-llama/notebook)\n",
    "\n",
    "### Change logs:\n",
    "-[version 6] (LB=0.??) used phi-2\n",
    "-[version 6] (LB=0.??) used Mistral-8x7b   \n",
    "-[version 5] (LB=0.??) used Mistral-7b   \n",
    "-[version 4] (LB=0.61) used Gemma-7b   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dedb2645",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T02:31:09.975072Z",
     "iopub.status.busy": "2024-03-29T02:31:09.974751Z",
     "iopub.status.idle": "2024-03-29T02:32:05.784330Z",
     "shell.execute_reply": "2024-03-29T02:32:05.783298Z"
    },
    "papermill": {
     "duration": 55.821874,
     "end_time": "2024-03-29T02:32:05.786818",
     "exception": false,
     "start_time": "2024-03-29T02:31:09.964944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/making-wheels-of-necessary-packages-for-hf-llms\r\n",
      "Processing /kaggle/input/making-wheels-of-necessary-packages-for-hf-llms/bitsandbytes-0.42.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.42.0) (1.11.4)\r\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from scipy->bitsandbytes==0.42.0) (1.26.4)\r\n",
      "Installing collected packages: bitsandbytes\r\n",
      "Successfully installed bitsandbytes-0.42.0\r\n",
      "Looking in links: /kaggle/input/making-wheels-of-necessary-packages-for-hf-llms\r\n",
      "Processing /kaggle/input/making-wheels-of-necessary-packages-for-hf-llms/accelerate-0.27.2-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2) (21.3)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2) (5.9.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2) (6.0.1)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2) (2.1.2)\r\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2) (0.20.3)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.27.2) (0.4.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.27.2) (3.1.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.27.2) (2024.2.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.27.2) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.27.2) (4.66.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.27.2) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.27.2) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.27.2) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.27.2) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.27.2) (2024.2.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.27.2) (1.3.0)\r\n",
      "accelerate is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "Looking in links: /kaggle/input/making-wheels-of-necessary-packages-for-hf-llms\r\n",
      "Processing /kaggle/input/making-wheels-of-necessary-packages-for-hf-llms/transformers-4.38.1-py3-none-any.whl\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.1) (3.13.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.1) (0.20.3)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.1) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.1) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.1) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.1) (2023.12.25)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.1) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.1) (0.15.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.1) (0.4.2)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38.1) (4.66.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.1) (2024.2.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.1) (4.9.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.38.1) (3.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.1) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.1) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.1) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38.1) (2024.2.2)\r\n",
      "transformers is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "Looking in links: /kaggle/input/making-wheels-of-necessary-packages-for-hf-llms\r\n",
      "Processing /kaggle/input/making-wheels-of-necessary-packages-for-hf-llms/optimum-1.17.1-py3-none-any.whl\r\n",
      "Processing /kaggle/input/making-wheels-of-necessary-packages-for-hf-llms/coloredlogs-15.0.1-py2.py3-none-any.whl (from optimum==1.17.1)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum==1.17.1) (1.12)\r\n",
      "Requirement already satisfied: transformers>=4.26.0 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum==1.17.1) (4.38.1)\r\n",
      "Requirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.10/site-packages (from optimum==1.17.1) (2.1.2)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from optimum==1.17.1) (21.3)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optimum==1.17.1) (1.26.4)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from optimum==1.17.1) (0.20.3)\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum==1.17.1) (2.1.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum==1.17.1) (3.13.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum==1.17.1) (2024.2.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum==1.17.1) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum==1.17.1) (4.66.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum==1.17.1) (6.0.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum==1.17.1) (4.9.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->optimum==1.17.1) (3.1.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.17.1) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.17.1) (3.1.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.26.0->transformers[sentencepiece]>=4.26.0->optimum==1.17.1) (2023.12.25)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.26.0->transformers[sentencepiece]>=4.26.0->optimum==1.17.1) (0.15.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.26.0->transformers[sentencepiece]>=4.26.0->optimum==1.17.1) (0.4.2)\r\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum==1.17.1) (0.2.0)\r\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum==1.17.1) (3.20.3)\r\n",
      "Processing /kaggle/input/making-wheels-of-necessary-packages-for-hf-llms/humanfriendly-10.0-py2.py3-none-any.whl (from coloredlogs->optimum==1.17.1)\r\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.17.1) (11.0.0)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.17.1) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.17.1) (2.1.4)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.17.1) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.17.1) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.17.1) (3.9.1)\r\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.17.1) (0.18.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum==1.17.1) (1.3.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.17.1) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.17.1) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.17.1) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.17.1) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.17.1) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum==1.17.1) (4.0.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum==1.17.1) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum==1.17.1) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum==1.17.1) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum==1.17.1) (2024.2.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum==1.17.1) (2.1.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.17.1) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.17.1) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.17.1) (2023.4)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum==1.17.1) (1.16.0)\r\n",
      "Installing collected packages: humanfriendly, coloredlogs, optimum\r\n",
      "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 optimum-1.17.1\r\n"
     ]
    }
   ],
   "source": [
    "# Install bitsandbytes\n",
    "!pip install --no-index /kaggle/input/making-wheels-of-necessary-packages-for-hf-llms/bitsandbytes-0.42.0-py3-none-any.whl --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-hf-llms\n",
    "!pip install --no-index /kaggle/input/making-wheels-of-necessary-packages-for-hf-llms/accelerate-0.27.2-py3-none-any.whl --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-hf-llms\n",
    "!pip install --no-index /kaggle/input/making-wheels-of-necessary-packages-for-hf-llms/transformers-4.38.1-py3-none-any.whl --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-hf-llms\n",
    "!pip install --no-index /kaggle/input/making-wheels-of-necessary-packages-for-hf-llms/optimum-1.17.1-py3-none-any.whl --find-links=/kaggle/input/making-wheels-of-necessary-packages-for-hf-llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbca7824",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T02:32:05.812130Z",
     "iopub.status.busy": "2024-03-29T02:32:05.811832Z",
     "iopub.status.idle": "2024-03-29T02:32:38.416439Z",
     "shell.execute_reply": "2024-03-29T02:32:38.415246Z"
    },
    "papermill": {
     "duration": 32.619755,
     "end_time": "2024-03-29T02:32:38.418779",
     "exception": false,
     "start_time": "2024-03-29T02:32:05.799024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install sentence transfomer\n",
    "!pip install -Uq /kaggle/input/sentence-transformers-2-4-0/sentence_transformers-2.4.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ef1efe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T02:32:38.443879Z",
     "iopub.status.busy": "2024-03-29T02:32:38.443582Z",
     "iopub.status.idle": "2024-03-29T02:32:59.438746Z",
     "shell.execute_reply": "2024-03-29T02:32:59.437820Z"
    },
    "papermill": {
     "duration": 21.010189,
     "end_time": "2024-03-29T02:32:59.441083",
     "exception": false,
     "start_time": "2024-03-29T02:32:38.430894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 02:32:46.642788: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-29 02:32:46.642890: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-29 02:32:46.782168: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from string import Template\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os, random\n",
    "import torch\n",
    "from torch import nn\n",
    "# Transformer\n",
    "import transformers\n",
    "from transformers import (pipeline, AutoTokenizer, AutoModelForCausalLM, \n",
    "                          BitsAndBytesConfig, AutoConfig)\n",
    "# For quantization\n",
    "import bitsandbytes, accelerate, optimum\n",
    "from tqdm.notebook import tqdm\n",
    "# For sentence transformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8dbb76f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T02:32:59.467238Z",
     "iopub.status.busy": "2024-03-29T02:32:59.466578Z",
     "iopub.status.idle": "2024-03-29T02:32:59.473669Z",
     "shell.execute_reply": "2024-03-29T02:32:59.472715Z"
    },
    "papermill": {
     "duration": 0.021958,
     "end_time": "2024-03-29T02:32:59.475687",
     "exception": false,
     "start_time": "2024-03-29T02:32:59.453729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ctypes, gc\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "# Seed the same seed to all \n",
    "def seed_everything(seed=42):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "def clear_memory():\n",
    "    libc.malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "# Set the GPUs\n",
    "DEVICE = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ba5ab0",
   "metadata": {
    "papermill": {
     "duration": 0.011606,
     "end_time": "2024-03-29T02:32:59.500806",
     "exception": false,
     "start_time": "2024-03-29T02:32:59.489200",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load the model\n",
    "\n",
    "Quantization technique is used to reduce memory and computational costs by representing weights and activations with lower-precision data types like 8-bit integers (int8). T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37979e3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T02:32:59.525787Z",
     "iopub.status.busy": "2024-03-29T02:32:59.525421Z",
     "iopub.status.idle": "2024-03-29T02:32:59.532829Z",
     "shell.execute_reply": "2024-03-29T02:32:59.532001Z"
    },
    "papermill": {
     "duration": 0.021927,
     "end_time": "2024-03-29T02:32:59.534686",
     "exception": false,
     "start_time": "2024-03-29T02:32:59.512759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Comment/Uncomment and use as per wish\n",
    "def load_model(model_name):\n",
    "    if model_name == 'gemma-7b':\n",
    "        MODEL_PATH = \"/kaggle/input/gemma/transformers/7b-it/1\"\n",
    "    elif model_name == 'gemma-2b':\n",
    "        MODEL_PATH = \"/kaggle/input/gemma/transformers/2b-it/2\"\n",
    "    elif model_name == 'mistral-7b':\n",
    "        MODEL_PATH = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\n",
    "    elif model_name == 'mistral-8x7b':\n",
    "        MODEL_PATH = \"/kaggle/input/mixtral/pytorch/8x7b-instruct-v0.1-hf/1\"\n",
    "    elif model_name == 'phi-2':\n",
    "        MODEL_PATH = \"/kaggle/input/phi/transformers/2/1\"\n",
    "\n",
    "    # Load the model using quantization techniques for using less memory\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, model_max_length=3072)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # Load the model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        device_map=DEVICE,\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=quantization_config,\n",
    "    )\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a99ff28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T02:32:59.559546Z",
     "iopub.status.busy": "2024-03-29T02:32:59.559240Z",
     "iopub.status.idle": "2024-03-29T02:34:15.354438Z",
     "shell.execute_reply": "2024-03-29T02:34:15.353586Z"
    },
    "papermill": {
     "duration": 75.810353,
     "end_time": "2024-03-29T02:34:15.356875",
     "exception": false,
     "start_time": "2024-03-29T02:32:59.546522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49624153cc746cd90eb6d0da882fff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = load_model(model_name='phi-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59efc7b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T02:34:15.383133Z",
     "iopub.status.busy": "2024-03-29T02:34:15.382816Z",
     "iopub.status.idle": "2024-03-29T02:34:15.390587Z",
     "shell.execute_reply": "2024-03-29T02:34:15.389724Z"
    },
    "papermill": {
     "duration": 0.023311,
     "end_time": "2024-03-29T02:34:15.392597",
     "exception": false,
     "start_time": "2024-03-29T02:34:15.369286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perplexity is a metric that measures the quality of language models\n",
    "# Perplexity is calculated as the exponent of the loss obtained from the model.\n",
    "class PerplexityNN(nn.Module):\n",
    "    def __init__(self, reduce: bool = True):\n",
    "        super().__init__()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "        perplexity = []\n",
    "        for i in range(labels.shape[0]):\n",
    "            perplexity.append(self.loss_fn(shift_logits[i], shift_labels[i]))\n",
    "        perplexity = torch.stack(perplexity, dim=0) # We use stack instead of exp\n",
    "        #perplexity = torch.exp(perplexity)\n",
    "        if self.reduce:\n",
    "            perplexity = torch.mean(perplexity)\n",
    "        return perplexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5c84592",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T02:34:15.418778Z",
     "iopub.status.busy": "2024-03-29T02:34:15.418488Z",
     "iopub.status.idle": "2024-03-29T02:34:15.423037Z",
     "shell.execute_reply": "2024-03-29T02:34:15.422195Z"
    },
    "papermill": {
     "duration": 0.020095,
     "end_time": "2024-03-29T02:34:15.424845",
     "exception": false,
     "start_time": "2024-03-29T02:34:15.404750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rewrite_prompts = [\n",
    "    \"\"\"Please improve the following text using the writing style of [insert desired style here],\n",
    "       maintaining the original meaning but enhancing the clarity, elegance, and impact \n",
    "       by altering the tone, diction, and stylistic elements to match the specified style \n",
    "       while ensuring the core message remains intact.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "372e9b68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T02:34:15.451232Z",
     "iopub.status.busy": "2024-03-29T02:34:15.450555Z",
     "iopub.status.idle": "2024-03-29T02:34:15.461328Z",
     "shell.execute_reply": "2024-03-29T02:34:15.460483Z"
    },
    "papermill": {
     "duration": 0.026186,
     "end_time": "2024-03-29T02:34:15.463331",
     "exception": false,
     "start_time": "2024-03-29T02:34:15.437145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine the rewrite prompt with the data (original text, rewritten text) as a prompt\n",
    "def format_prompt(row, rw_prompt):\n",
    "    return f\"\"\"<start_of_turn>user {rw_prompt} {row[\"original_text\"]}<end_of_turn>\n",
    "                  <start_of_turn>model{row[\"rewritten_text\"]}<end_of_turn>\"\"\"\n",
    "\n",
    "# recover the promot for given texts (df)\n",
    "def recover_prompt(df, model, tokenizer):\n",
    "    perp_nn = PerplexityNN() # Compute the perplexity\n",
    "    preds = []\n",
    "    for idx in range(len(df)):\n",
    "        row = df.iloc[idx]\n",
    "        perps = []\n",
    "        with torch.no_grad():\n",
    "            formatted_prompts = []\n",
    "            for rw_prompt in rewrite_prompts:\n",
    "                formatted_prompts.append(format_prompt(row, rw_prompt))\n",
    "            # Encode prompts to embeddings\n",
    "            inputs = tokenizer(formatted_prompts, return_tensors=\"pt\",\n",
    "                               add_special_tokens=False,\n",
    "                               padding=True, truncation=True).to(\"cuda\")\n",
    "            # Get the output\n",
    "            output = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "            output = output.logits\n",
    "            \n",
    "            labels = inputs[\"input_ids\"]\n",
    "            labels.masked_fill_(~inputs[\"attention_mask\"].bool(), -100)\n",
    "            \n",
    "            # Compute the perplexity of model output (logits) and actual labels\n",
    "            for j in range(len(rewrite_prompts)):\n",
    "                p = perp_nn(output[j].unsqueeze(0), labels[j].unsqueeze(0))\n",
    "                perps.append(p.detach().cpu())\n",
    "            del inputs, labels, output, p\n",
    "        # Convert 'perps' as numpy array\n",
    "        perps = np.array(perps)\n",
    "        # Get the best output results of the lowest \n",
    "        fnal_pred = [np.array(rewrite_prompts)[np.argsort(perps)][0]]\n",
    "        preds.append(fnal_pred[0])\n",
    "        clear_memory()\n",
    "        print(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61896d59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T02:34:15.489459Z",
     "iopub.status.busy": "2024-03-29T02:34:15.489139Z",
     "iopub.status.idle": "2024-03-29T02:34:15.520705Z",
     "shell.execute_reply": "2024-03-29T02:34:15.519720Z"
    },
    "papermill": {
     "duration": 0.047011,
     "end_time": "2024-03-29T02:34:15.522877",
     "exception": false,
     "start_time": "2024-03-29T02:34:15.475866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>rewritten_text</th>\n",
       "      <th>rewrite_prompt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>The competition dataset comprises text passage...</td>\n",
       "      <td>Here is your shanty: (Verse 1) The text is rew...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        original_text  \\\n",
       "id                                                      \n",
       "-1  The competition dataset comprises text passage...   \n",
       "\n",
       "                                       rewritten_text rewrite_prompt  \n",
       "id                                                                    \n",
       "-1  Here is your shanty: (Verse 1) The text is rew...                 "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the testing data\n",
    "# train_df = pd.read_csv('/kaggle/input/llm-prompt-recovery'train.csv', index_col='id')\n",
    "test_df = pd.read_csv('/kaggle/input/llm-prompt-recovery/test.csv', index_col='id')\n",
    "test_df[\"rewrite_prompt\"] = \"\" # Empty\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "51a37436",
   "metadata": {
    "papermill": {
     "duration": 0.011969,
     "end_time": "2024-03-29T02:34:15.548774",
     "exception": false,
     "start_time": "2024-03-29T02:34:15.536805",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Compute the similarity between rewrite prompt and predicted prompts\n",
    "def CVScore(df):\n",
    "    scs = lambda row: abs((cosine_similarity(row[\"actual_embeddings\"], row[\"pred_embeddings\"])) ** 3)\n",
    "    # Load sentence transformer\n",
    "    model = SentenceTransformer('/kaggle/input/sentence-t5-base-hf/sentence-t5-base')\n",
    "    # Convert ret\n",
    "    df[\"actual_embeddings\"] = df[\"rewrite_prompt\"].progress_apply(lambda x: \n",
    "                                            model.encode(x, normalize_embeddings=True, show_progress_bar=False).reshape(1, -1))\n",
    "    df[\"pred_embeddings\"] = df[\"pred\"].progress_apply(lambda x: \n",
    "                                            model.encode(x, normalize_embeddings=True, show_progress_bar=False).reshape(1, -1))\n",
    "    \n",
    "    df[\"score\"] = df.apply(scs, axis=1)\n",
    "    \n",
    "    return np.mean(df['score'])[0][0]\n",
    "    \n",
    "print(f\"CV Score: {CVScore(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50569f7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T02:34:15.575052Z",
     "iopub.status.busy": "2024-03-29T02:34:15.574789Z",
     "iopub.status.idle": "2024-03-29T02:34:17.010892Z",
     "shell.execute_reply": "2024-03-29T02:34:17.009768Z"
    },
    "papermill": {
     "duration": 1.451366,
     "end_time": "2024-03-29T02:34:17.012967",
     "exception": false,
     "start_time": "2024-03-29T02:34:15.561601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Please improve the following text using the writing style of [insert desired style here],\\n       maintaining the original meaning but enhancing the clarity, elegance, and impact \\n       by altering the tone, diction, and stylistic elements to match the specified style \\n       while ensuring the core message remains intact.']\n"
     ]
    }
   ],
   "source": [
    "preds = recover_prompt(test_df, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f96ff186",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T02:34:17.039745Z",
     "iopub.status.busy": "2024-03-29T02:34:17.039179Z",
     "iopub.status.idle": "2024-03-29T02:34:17.054630Z",
     "shell.execute_reply": "2024-03-29T02:34:17.053792Z"
    },
    "papermill": {
     "duration": 0.030868,
     "end_time": "2024-03-29T02:34:17.056585",
     "exception": false,
     "start_time": "2024-03-29T02:34:17.025717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rewrite_prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9559194</td>\n",
       "      <td>Please improve the following text using the wr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                     rewrite_prompt\n",
       "0  9559194  Please improve the following text using the wr..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "submission = pd.read_csv('/kaggle/input/llm-prompt-recovery/sample_submission.csv')\n",
    "submission[\"rewrite_prompt\"] = preds\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "display(submission)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7806901,
     "sourceId": 67121,
     "sourceType": "competition"
    },
    {
     "datasetId": 4506214,
     "sourceId": 7747717,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4516155,
     "sourceId": 7729151,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 164836055,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 3900,
     "sourceId": 5112,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 4761,
     "sourceId": 5994,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 8658,
     "sourceId": 10716,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 8332,
     "sourceId": 11261,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 8318,
     "sourceId": 11382,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 193.267778,
   "end_time": "2024-03-29T02:34:20.420025",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-29T02:31:07.152247",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2b31243005474e9fba7e4cf5942243f2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3702b2c76478488c9b569a0d30b697c1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5487fb5717814bbcb0bfe93039af38df": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3702b2c76478488c9b569a0d30b697c1",
       "placeholder": "​",
       "style": "IPY_MODEL_5580ba5afbdd433e96546ca1554a92ef",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "5580ba5afbdd433e96546ca1554a92ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "57f9342874c74df883be42a97ab0f5e3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6bcea5bf0af94c42800d07e596e97bba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7899500dd4cb4bb39498b7dcaf57a023": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2b31243005474e9fba7e4cf5942243f2",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_867990b081ee441cbdd5c6400312b68c",
       "value": 2.0
      }
     },
     "867990b081ee441cbdd5c6400312b68c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a71d7059ea29406d83e3225f781181bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d49624153cc746cd90eb6d0da882fff4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5487fb5717814bbcb0bfe93039af38df",
        "IPY_MODEL_7899500dd4cb4bb39498b7dcaf57a023",
        "IPY_MODEL_f21c082060c4483cb4f3f3d03b6f740e"
       ],
       "layout": "IPY_MODEL_6bcea5bf0af94c42800d07e596e97bba"
      }
     },
     "f21c082060c4483cb4f3f3d03b6f740e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_57f9342874c74df883be42a97ab0f5e3",
       "placeholder": "​",
       "style": "IPY_MODEL_a71d7059ea29406d83e3225f781181bc",
       "value": " 2/2 [01:14&lt;00:00, 32.16s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
